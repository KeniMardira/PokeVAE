{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *\n",
    "BATCH_SIZE = hyper_parameters['BATCH_SIZE']\n",
    "PREPROCESS = True\n",
    "FLIP = False\n",
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pokemon Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROCESS:\n",
    "    try:\n",
    "        os.system(\"rm ./Pokemon/flipped_*\")\n",
    "    except:\n",
    "        print(\"No flipped images\")\n",
    "\n",
    "    create_csv()\n",
    "    if FLIP:\n",
    "        pokelist = pd.read_csv(\"./PokeList\", header=None)\n",
    "        for i in range(851):\n",
    "            pokemon=np.fliplr(mpimg.imread(\"./Pokemon/\"+ pokelist[i][0]))\n",
    "            plt.imsave(\"./PokemonFlipped/flipped_\" + pokelist[i][0][:-4] + \".jpg\", pokemon)\n",
    "        os.system(\"mv ./PokemonFlipped/* ./Pokemon/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROCESS:\n",
    "    create_csv()\n",
    "pokeSet = PokeSet(\"./Pokemon/\")\n",
    "pokeLoader = Utils.DataLoader(dataset = pokeSet, shuffle = True, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROCESS:\n",
    "    create_obamacsv()\n",
    "obamaSet = ObamaSet(\"./images/\")\n",
    "obamaLoader = Utils.DataLoader(dataset = obamaSet, shuffle = True, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "label = \"L20B10\"\n",
    "row = 4\n",
    "col = 4\n",
    "\n",
    "losses = []\n",
    "bces = []\n",
    "kls = []\n",
    "def train(model,optimizer,dataloader, epoch, max_epochs=5000000):\n",
    "    \n",
    "    for _ in range(max_epochs):\n",
    "        epoch += 1\n",
    "        for images in dataloader:\n",
    "            image_in = images.permute(0,3,1,2)\n",
    "            x_in = Variable(image_in.float()).cuda()\n",
    "            optimizer.zero_grad()\n",
    "#             x_out, z_mu, z_logvar = model(x_in)\n",
    "            loss, bce, kl = model.calculate_loss(x_in)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            bces.append(bce.item())\n",
    "            kls.append(kl.item())\n",
    "            \n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            save_file = label + \"_epoch_{:06d}\".format(epoch) + '.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'losses' : losses\n",
    "            }, \"checkpoints/\" + save_file)\n",
    "                \n",
    "        clear_output(wait=True)\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                pokePlot(images, model, row, col, epoch)\n",
    "        print(epoch, 'Loss: {:3f}'.format(loss.item()))\n",
    "        \n",
    "    return losses, bces, kls\n",
    "try:\n",
    "    net, epoch, losses, optimizer = load_checkpoint(\"./checkpoints/L20B10_epoch_000100.pthjj\")\n",
    "except:\n",
    "    net = Net().cuda()\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "    epoch = 0\n",
    "    \n",
    "train_losses, bces, kls = train(net,optimizer,obamaLoader, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(bces)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(kls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = mpimg.imread(\"./images/00039-00295.jpg\")\n",
    "if pokemon.shape[-1] == 4:\n",
    "    pokemon = pokemon[:,:,:3]\n",
    "pokemon = cv2.resize(pokemon, (RESIZE,RESIZE))\n",
    "pokemon = (pokemon - np.min(pokemon))/np.max(pokemon - np.min(pokemon))\n",
    "\n",
    "# pokemon = np.rot90(pokemon, axes=[0,1]).copy()\n",
    "# plt.imshow(pokemon)\n",
    "# print(pokemon)\n",
    "\n",
    "image = torch.from_numpy(pokemon).unsqueeze(0)\n",
    "image = image[0,:,:,:].unsqueeze(0)\n",
    "plt.imshow((image.squeeze().numpy()))\n",
    "plt.show()\n",
    "image = image.permute(0,3,1,2)\n",
    "x_out, z_mean, z_logvar = net(Variable(image.float()).cuda())\n",
    "x_out = x_out.permute(0,2,3,1)\n",
    "plt.imshow(x_out.data.cpu().squeeze().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(image, dim, min_range, max_range, step):\n",
    "#     plt.imshow((image.squeeze().numpy()))\n",
    "#     plt.show()\n",
    "    image = image.permute(0,3,1,2)\n",
    "    z_mean, z_logvar = net.encoder(Variable(image.float()).cuda())\n",
    "    z = net.latent(z_mean,z_logvar)\n",
    "    for i, sw in enumerate(range(min_range, max_range, step)):\n",
    "        z[0][dim] = sw/10\n",
    "        x_out = net.decoder(z)\n",
    "        x_out = x_out.permute(0,2,3,1)\n",
    "        im = (np.floor(x_out.data.cpu().squeeze().numpy() * 255)).astype(np.uint8)\n",
    "        plt.imsave(\"./sweep/\" + str(dim) + \"/\" + \"{0:03d}\".format(i) + \".png\", im)\n",
    "    generate_animation(\"./sweep/\" + str(dim) + \"/\", dim, (max_range - min_range)//step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation(path, dim, num):\n",
    "    images = []\n",
    "    for e in range(num):\n",
    "        img_name = path + \"{0:03d}\".format(e) + '.png'\n",
    "        images.append(mpimg.imread(img_name))\n",
    "    imageio.mimsave(\"./sweep/gif/\" + 'dim' + str(dim) + '_generate_animation.gif', images, fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.permute(0,2,3,1)\n",
    "os.system(\"rm -r ./sweep/*\")\n",
    "os.system(\"mkdir ./sweep/gif/\")\n",
    "for i in range(LATENT_DIM):\n",
    "    os.system(\"mkdir ./sweep/\"+str(i))\n",
    "    sweep(image, i, -50, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_linear(path1, path2, savepath, STEP):\n",
    "    pokemon = mpimg.imread(path1)\n",
    "    if pokemon.shape[-1] == 4:\n",
    "        pokemon = pokemon[:,:,:3]\n",
    "    pokemon = cv2.resize(pokemon, (RESIZE,RESIZE))\n",
    "    pokemon = (pokemon - np.min(pokemon))/np.max(pokemon - np.min(pokemon))\n",
    "    image = torch.from_numpy(pokemon).unsqueeze(0)\n",
    "\n",
    "    image = image[0,:,:,:].unsqueeze(0)\n",
    "    image = image.permute(0,3,1,2)\n",
    "    z_mean, z_logvar = net.encoder(Variable(image.float()).cuda())\n",
    "    z1 = net.latent(z_mean, z_logvar)\n",
    "    ss = net.decoder(z1)\n",
    "    ss = ss.permute(0,2,3,1)\n",
    "    plt.imshow(ss.data.cpu().squeeze().numpy())\n",
    "    plt.show()\n",
    "\n",
    "    pokemon = mpimg.imread(path2)\n",
    "    if pokemon.shape[-1] == 4:\n",
    "        pokemon = pokemon[:,:,:3]\n",
    "    pokemon = cv2.resize(pokemon, (RESIZE,RESIZE))\n",
    "    pokemon = (pokemon - np.min(pokemon))/np.max(pokemon - np.min(pokemon))\n",
    "    image = torch.from_numpy(pokemon).unsqueeze(0)\n",
    "\n",
    "    image = image[0,:,:,:].unsqueeze(0)\n",
    "    image = image.permute(0,3,1,2)\n",
    "    z_mean, z_logvar = net.encoder(Variable(image.float()).cuda())\n",
    "    z2 = net.latent(z_mean, z_logvar)\n",
    "    ss = net.decoder(z2)\n",
    "    ss = ss.permute(0,2,3,1)\n",
    "    plt.imshow(ss.data.cpu().squeeze().numpy())\n",
    "    plt.show()\n",
    "    step = (z2-z1)/STEP\n",
    "\n",
    "    for i in range(STEP+1):\n",
    "        ss = net.decoder(z1 + i*step)\n",
    "        ss = ss.permute(0,2,3,1)\n",
    "        plt.imsave(savepath + \"{0:03d}\".format(i) + \".png\", ss.data.cpu().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation_interpol(path, name, num):\n",
    "    images = []\n",
    "    for e in range(num):\n",
    "        img_name = path + \"{0:03d}\".format(e) + '.png'\n",
    "        images.append(mpimg.imread(img_name))\n",
    "    imageio.mimsave(\"./interpolate/gif/\" + name + '_generate_animation.gif', images, fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 100\n",
    "poke1 = \"./images/00039-00295.jpg\"\n",
    "poke2 = \"./images/00039-00396.jpg\"\n",
    "label = poke1[10:-4] + \"_\" + poke2[10:-4]\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "    if not (os.path.exists(\"./interpolate/\" + label)):\n",
    "        os.system(\"mkdir \" + \"./interpolate/\" + label)\n",
    "        print(label)\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "        if label [-2] == \"_\":\n",
    "            label = label[:-2]\n",
    "        label = label + \"_\" + str(i)\n",
    "        \n",
    "interpolate_linear(poke1, poke2, \"./interpolate/\" + label + \"/\", STEP)\n",
    "generate_animation_interpol(\"./interpolate/\" + label + \"/\", label, STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test data encodings on the latent space\n",
    "def visualize_encoder(model,dataloader):\n",
    "    z_means_x, z_means_y, all_labels = [], [], []\n",
    "    \n",
    "    for images in iter(dataloader):\n",
    "        images = images.permute(0,3,1,2)\n",
    "        z_means,_ = model.encoder(Variable(images.float()))\n",
    "        z_means_x = np.append(z_means_x,z_means[:,0].data.numpy())\n",
    "        z_means_y = np.append(z_means_y,z_means[:,1].data.numpy())\n",
    "#         all_labels = np.append(all_labels,labels.numpy())\n",
    "        \n",
    "    plt.figure(figsize=(6.5,5))\n",
    "    plt.scatter(z_means_x,z_means_y,cmap='inferno')\n",
    "    plt.show()\n",
    "\n",
    "visualize_encoder(net,pokeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize moving average of losses\n",
    "def visualize_losses_moving_average(losses,window=50,boundary='valid'):\n",
    "    mav_losses = np.convolve(losses,np.ones(window)/window,boundary)\n",
    "    corrected_mav_losses = np.append(np.full(window-1,np.nan),mav_losses)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(losses)\n",
    "    plt.plot(corrected_mav_losses)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n",
    "\n",
    "visualize_losses_moving_average(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, _ = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs = Variable(inputs)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        x_out, z_mean, z_logvar = net(inputs)\n",
    "        loss = criterion(x_out, inputs, z_mean, z_logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "#Predict with network\n",
    "outputs, _, _ = net(Variable(images))\n",
    "\n",
    "# _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "#                               for j in range(4)))\n",
    "\n",
    "\n",
    "imshow(torchvision.utils.make_grid(outputs.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test network accuracy\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs, _, _ = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from scipy.stats import norm\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid as make_image_grid\n",
    "from tqdm import tnrange\n",
    "\n",
    "torch.manual_seed(2017) # reproducability\n",
    "sns.set_style('dark')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim=20,hidden_dim=500):\n",
    "        super(VAE,self).__init__()\n",
    "        self.fc_e = nn.Linear(784,hidden_dim)\n",
    "        self.fc_mean = nn.Linear(hidden_dim,latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim,latent_dim)\n",
    "        self.fc_d1 = nn.Linear(latent_dim,hidden_dim)\n",
    "        self.fc_d2 = nn.Linear(hidden_dim,784)\n",
    "            \n",
    "    def encoder(self,x_in):\n",
    "        x = F.relu(self.fc_e(x_in.view(-1,784)))\n",
    "        mean = self.fc_mean(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def decoder(self,z):\n",
    "        z = F.relu(self.fc_d1(z))\n",
    "        x_out = F.sigmoid(self.fc_d2(z))\n",
    "        return x_out.view(-1,1,28,28)\n",
    "    \n",
    "    def sample_normal(self,mean,logvar):\n",
    "        # Using torch.normal(means,sds) returns a stochastic tensor which we cannot backpropogate through.\n",
    "        # Instead we utilize the 'reparameterization trick'.\n",
    "        # http://stats.stackexchange.com/a/205336\n",
    "        # http://dpkingma.com/wordpress/wp-content/uploads/2015/12/talk_nips_workshop_2015.pdf\n",
    "        sd = torch.exp(logvar*0.5)\n",
    "        e = Variable(torch.randn(sd.size())) # Sample from standard normal\n",
    "        z = e.mul(sd).add_(mean)\n",
    "        return z\n",
    "    \n",
    "    def forward(self,x_in):\n",
    "        z_mean, z_logvar = self.encoder(x_in)\n",
    "        z = self.sample_normal(z_mean,z_logvar)\n",
    "        x_out = self.decoder(z)\n",
    "        return x_out, z_mean, z_logvar\n",
    "\n",
    "model = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def criterion(x_out,x_in,z_mu,z_logvar):\n",
    "    bce_loss = F.binary_cross_entropy(x_out,x_in,size_average=False)\n",
    "    kld_loss = -0.5 * torch.sum(1 + z_logvar - (z_mu ** 2) - torch.exp(z_logvar))\n",
    "    loss = (bce_loss + kld_loss) / x_out.size(0) # normalize by batch size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "trainloader = DataLoader(\n",
    "    MNIST(root='./MNIST',train=True,download=True,transform=transforms.ToTensor()),\n",
    "    batch_size=128,shuffle=True)\n",
    "testloader = DataLoader(\n",
    "    MNIST(root='./MNIST',train=False,download=True,transform=transforms.ToTensor()),\n",
    "    batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model,optimizer,dataloader,epochs=15):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for images,_ in dataloader:\n",
    "            x_in = Variable(images)\n",
    "            optimizer.zero_grad()\n",
    "            x_out, z_mu, z_logvar = model(x_in)\n",
    "            loss = criterion(x_out,x_in,z_mu,z_logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data[0])\n",
    "        print('Loss: {:4f}'.format(loss.data[0]))\n",
    "    return losses\n",
    "\n",
    "train_losses = train(model,optimizer,trainloader)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_source]",
   "language": "python",
   "name": "conda-env-pytorch_source-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
